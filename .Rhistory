source_python("api_clients/aa_summarization.py")
summary2 = summary(token, document, as.integer(maximum_tokens))
print(summary2)
# Part 3
part3 = round(nrow(group)/4)*3+1
stringtosum3 = ""
for (x in part2+1:part3) {
stringtosum3 = paste(stringtosum3, group[x,1], sep = " ", collapse = " ")
tokensize = count_tokens(stringtosum3)
}
document = stringtosum3
source_python("api_clients/aa_summarization.py")
summary3 = summary(token, document, as.integer(maximum_tokens))
print(summary3)
# Part 4
part4 = nrow(group)
stringtosum4 = ""
for (x in part3+1:nrow(group)) {
stringtosum4 = paste(stringtosum4, group[x,1], sep = " ", collapse = " ")
tokensize = count_tokens(stringtosum4)
}
document = stringtosum4
source_python("api_clients/aa_summarization.py")
summary4 = summary(token, document, as.integer(maximum_tokens))
print(summary4)
# All
document = paste(summary1, summary2, summary3, summary4, sep = " ", collapse = " ")
source_python("api_clients/aa_summarization.py")
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
summarydf = rbind(summarydf, summary)
} else {
print(glue("------------------------------------------------------------Routine"))
document = stringtosum
source_python("api_clients/aa_summarization.py")
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
summarydf = rbind(summarydf, summary)
}
print(glue("------------------------------------------------------------Next"))
colnames(summarydf) = c("Summaries")
}
maximum_tokens = 500
summarydf = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:groupsize) {
#x = 1
group = df2[df2$cluster == x,]
group
stringtosum = ""
for (y in 1:nrow(group)) {
stringtosum = paste(stringtosum, group[y,3], sep = " ", collapse = " ")
}
tokensize = count_tokens(stringtosum)
tokensize
print(glue("------------------------------------------------------------Current group is: {x} with tokensize {tokensize}."))
if (tokensize > 1500) {
print(glue("------------------------------------------------------------Subroutine"))
# Part 1
part1 = 1
stringtosum1 = ""
for (x in 1:part1) {
stringtosum1 = paste(stringtosum1, group[x,1], sep = " ", collapse = " ")
tokensize = count_tokens(stringtosum1)
}
document = stringtosum1
tokensize1 = count_tokens(stringtosum1)
source_python("api_clients/aa_summarization.py")
summary1 = summary(token, document, as.integer(maximum_tokens))
print(summary1)
# Part 2
part2 = round(nrow(group)/4)*2
stringtosum2 = ""
for (x in part1+1:part2) {
stringtosum2 = paste(stringtosum2, group[x,1], sep = " ", collapse = " ")
tokensize = count_tokens(stringtosum2)
}
document = stringtosum2
source_python("api_clients/aa_summarization.py")
summary2 = summary(token, document, as.integer(maximum_tokens))
print(summary2)
# Part 3
part3 = round(nrow(group)/4)*3+1
stringtosum3 = ""
for (x in part2+1:part3) {
stringtosum3 = paste(stringtosum3, group[x,1], sep = " ", collapse = " ")
tokensize = count_tokens(stringtosum3)
}
document = stringtosum3
source_python("api_clients/aa_summarization.py")
summary3 = summary(token, document, as.integer(maximum_tokens))
print(summary3)
# Part 4
part4 = nrow(group)
stringtosum4 = ""
for (x in part3+1:nrow(group)) {
stringtosum4 = paste(stringtosum4, group[x,1], sep = " ", collapse = " ")
tokensize = count_tokens(stringtosum4)
}
document = stringtosum4
source_python("api_clients/aa_summarization.py")
summary4 = summary(token, document, as.integer(maximum_tokens))
print(summary4)
# All
document = paste(summary1, summary2, summary3, summary4, sep = " ", collapse = " ")
source_python("api_clients/aa_summarization.py")
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
summarydf = rbind(summarydf, summary)
} else {
print(glue("------------------------------------------------------------Routine"))
document = stringtosum
source_python("api_clients/aa_summarization.py")
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
summarydf = rbind(summarydf, summary)
}
print(glue("------------------------------------------------------------Next"))
colnames(summarydf) = c("Summaries")
}
maximum_tokens = 500
summarydf = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:groupsize) {
#x = 1
group = df2[df2$cluster == x,]
group
stringtosum = ""
for (y in 1:nrow(group)) {
stringtosum = paste(stringtosum, group[y,3], sep = " ", collapse = " ")
}
tokensize = count_tokens(stringtosum)
tokensize
print(glue("------------------------------------------------------------Current group is: {x} with tokensize {tokensize}."))
if (tokensize > 1500) {
print(glue("------------------------------------------------------------Subroutine"))
# Part 1
part1 = 1
stringtosum1 = ""
for (x in 1:part1) {
stringtosum1 = paste(stringtosum1, group[x,1], sep = " ", collapse = " ")
tokensize = count_tokens(stringtosum1)
}
document = stringtosum1
tokensize1 = count_tokens(stringtosum1)
summary1 = summary(token, document, as.integer(maximum_tokens))
print(summary1)
# Part 2
part2 = round(nrow(group)/4)*2
stringtosum2 = ""
for (x in part1+1:part2) {
stringtosum2 = paste(stringtosum2, group[x,1], sep = " ", collapse = " ")
tokensize = count_tokens(stringtosum2)
}
document = stringtosum2
summary2 = summary(token, document, as.integer(maximum_tokens))
print(summary2)
# Part 3
part3 = round(nrow(group)/4)*3+1
stringtosum3 = ""
for (x in part2+1:part3) {
stringtosum3 = paste(stringtosum3, group[x,1], sep = " ", collapse = " ")
tokensize = count_tokens(stringtosum3)
}
document = stringtosum3
summary3 = summary(token, document, as.integer(maximum_tokens))
print(summary3)
# Part 4
part4 = nrow(group)
stringtosum4 = ""
for (x in part3+1:nrow(group)) {
stringtosum4 = paste(stringtosum4, group[x,1], sep = " ", collapse = " ")
tokensize = count_tokens(stringtosum4)
}
document = stringtosum4
summary4 = summary(token, document, as.integer(maximum_tokens))
print(summary4)
# All
document = paste(summary1, summary2, summary3, summary4, sep = " ", collapse = " ")
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
summarydf = rbind(summarydf, summary)
} else {
print(glue("------------------------------------------------------------Routine"))
document = stringtosum
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
summarydf = rbind(summarydf, summary)
}
print(glue("------------------------------------------------------------Next"))
colnames(summarydf) = c("Summaries")
}
# Get your python file
source_python("aa_summarization.py")
maximum_tokens = 500
summarydf = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:groupsize) {
#x = 1
group = df2[df2$cluster == x,]
group
stringtosum = ""
for (y in 1:nrow(group)) {
stringtosum = paste(stringtosum, group[y,3], sep = " ", collapse = " ")
}
tokensize = count_tokens(stringtosum)
tokensize
print(glue("------------------------------------------------------------Current group is: {x} with tokensize {tokensize}."))
if (tokensize > 1500) {
print(glue("------------------------------------------------------------Subroutine"))
# Part 1
part1 = 1
stringtosum1 = ""
for (x in 1:part1) {
stringtosum1 = paste(stringtosum1, group[x,1], sep = " ", collapse = " ")
tokensize = count_tokens(stringtosum1)
}
document = stringtosum1
tokensize1 = count_tokens(stringtosum1)
summary1 = summary(token, document, as.integer(maximum_tokens))
print(summary1)
# Part 2
part2 = round(nrow(group)/4)*2
stringtosum2 = ""
for (x in part1+1:part2) {
stringtosum2 = paste(stringtosum2, group[x,1], sep = " ", collapse = " ")
tokensize = count_tokens(stringtosum2)
}
document = stringtosum2
summary2 = summary(token, document, as.integer(maximum_tokens))
print(summary2)
# Part 3
part3 = round(nrow(group)/4)*3+1
stringtosum3 = ""
for (x in part2+1:part3) {
stringtosum3 = paste(stringtosum3, group[x,1], sep = " ", collapse = " ")
tokensize = count_tokens(stringtosum3)
}
document = stringtosum3
summary3 = summary(token, document, as.integer(maximum_tokens))
print(summary3)
# Part 4
part4 = nrow(group)
stringtosum4 = ""
for (x in part3+1:nrow(group)) {
stringtosum4 = paste(stringtosum4, group[x,1], sep = " ", collapse = " ")
tokensize = count_tokens(stringtosum4)
}
document = stringtosum4
summary4 = summary(token, document, as.integer(maximum_tokens))
print(summary4)
# All
document = paste(summary1, summary2, summary3, summary4, sep = " ", collapse = " ")
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
summarydf = rbind(summarydf, summary)
} else {
print(glue("------------------------------------------------------------Routine"))
document = stringtosum
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
summarydf = rbind(summarydf, summary)
}
print(glue("------------------------------------------------------------Next"))
colnames(summarydf) = c("Summaries")
}
View(df2)
View(presummarydf)
presummarydf
View(df2)
df2
# Install and or load packages - set variables ---------------------------------
packages <- c("pdftools","reticulate")
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
library(pdftools)
library(reticulate)
library(TheOpenAIR)
library(ds4psy)
library(glue)
# Initialize virtual ENV for Python
Sys.setenv(RETICULATE_PYTHON_ENV = "py_backend")
version <- "3.11"
virtualenv_create(python = virtualenv_starter(version))
virtualenv_install(requirements = "requirements.txt")
use_virtualenv("py_backend")
# Get your python file
source_python("aa_summarization.py")
source_python("aa_embedding.py")
outputpath = getwd()
inputpath = "Testfiles/1.pdf"
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzU1MH0.lqb0stSnX59IihBKdwISywfgK4amb6pBuhCNCSZqMnA"
################################################################################
txt = pdf_text(inputpath)
maximum_tokens = 124
# processing of PDF file
stringtosum = ""
for (x in 1:length(txt)) {
stringtosum = paste(stringtosum, txt[x], sep = " ", collapse = " ")
}
sumtokeninput = count_tokens(stringtosum)
dft = data.frame(page="",
tokens=""
)
x = 1
for (x in 1:length(txt)) {
tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
dft = dft[-1,]
dft$tokens <- as.integer(dft$tokens)
sumtokeninput <- sum(dft[, 'tokens'])
# Chunking algorithm -----------------------------------------------------------
string = paste(txt, sep="", collapse="")
string = gsub("[\r\n]", "", string) # remove new lines
dftext = text_to_sentences(string, split_delim = "\\.")  # only split at "."
dftext = as.data.frame(dftext)
chunksize = 5
overlap = 1
cutsize = chunksize - overlap
iterations = round(nrow(dftext)/cutsize)-1
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:iterations){
chunk = paste(dftext[c(1:chunksize),], sep="", collapse="")
df = rbind(df, chunk)
dftext = as.data.frame(dftext[-c(1:cutsize), ])
}
if (nrow(dftext) > 0) {
chunk = paste(dftext[c(1:nrow(dftext)),], sep="", collapse="")
df = rbind(df, chunk)
}
colnames(df) <- c("Text_chunk")
text_chunks = as.list(df[,1])
# Clustering algorithm ---------------------------------------------------------
vectors = embedding(token, text_chunks)
vectors
vectors = matrix(unlist(vectors), ncol = 5120, byrow = TRUE)
# Chunking algorithm and pre-summary -------------------------------------------
set.seed(123)
groupsize = 5
km.res <- kmeans(vectors, groupsize, nstart = 25)
df2 <- cbind(df, cluster = km.res$cluster)
presummarydf = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(df2)) {
document = df2[x,1]
source_python("aa_summarization.py")
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
presummarydf = rbind(presummarydf, summary)
}
for (x in 1:nrow(df2)) {
document = df2[x,1]
source_python("aa_summarization.py")
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
presummarydf = rbind(presummarydf, summary)
}
for (x in 1:nrow(presummarydf)) {
stringtosum = paste(stringtosum, presummarydf[x,1], sep = " ", collapse = " ")
}
sumtokenoutput = 0
sumtokenoutput = sumtokenoutput + count_tokens(stringtosum)
df2 <- cbind(df2, presummarydf)
colnames(df2) <- c("Text_chunk", "cluster", "Summary")
View(df2)
# Install and or load packages - set variables ---------------------------------
packages <- c("pdftools","reticulate")
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
library(pdftools)
library(reticulate)
library(TheOpenAIR)
library(ds4psy)
library(glue)
# Initialize virtual ENV for Python
Sys.setenv(RETICULATE_PYTHON_ENV = "py_backend")
version <- "3.11"
virtualenv_create(python = virtualenv_starter(version))
virtualenv_install(requirements = "requirements.txt")
use_virtualenv("py_backend")
# Get your python file
source_python("aa_summarization.py")
source_python("aa_embedding.py")
outputpath = getwd()
inputpath = "Testfiles/1.pdf"
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzU1MH0.lqb0stSnX59IihBKdwISywfgK4amb6pBuhCNCSZqMnA"
txt = pdf_text(inputpath)
maximum_tokens = 124
# processing of PDF file
stringtosum = ""
for (x in 1:length(txt)) {
stringtosum = paste(stringtosum, txt[x], sep = " ", collapse = " ")
}
sumtokeninput = count_tokens(stringtosum)
dft = data.frame(page="",
tokens=""
)
x = 1
for (x in 1:length(txt)) {
tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
dft = dft[-1,]
dft$tokens <- as.integer(dft$tokens)
sumtokeninput <- sum(dft[, 'tokens'])
# Chunking algorithm -----------------------------------------------------------
string = paste(txt, sep="", collapse="")
string = gsub("[\r\n]", "", string) # remove new lines
dftext = text_to_sentences(string, split_delim = "\\.")  # only split at "."
dftext = as.data.frame(dftext)
chunksize = 5
overlap = 1
cutsize = chunksize - overlap
iterations = round(nrow(dftext)/cutsize)-1
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:iterations){
chunk = paste(dftext[c(1:chunksize),], sep="", collapse="")
df = rbind(df, chunk)
dftext = as.data.frame(dftext[-c(1:cutsize), ])
}
if (nrow(dftext) > 0) {
chunk = paste(dftext[c(1:nrow(dftext)),], sep="", collapse="")
df = rbind(df, chunk)
}
colnames(df) <- c("Text_chunk")
text_chunks = as.list(df[,1])
# Clustering algorithm ---------------------------------------------------------
vectors = embedding(token, text_chunks)
vectors
vectors = matrix(unlist(vectors), ncol = 5120, byrow = TRUE)
#fviz_nbclust(vectors, kmeans, method = "wss") +
#  geom_vline(xintercept = "", linetype = 2)
# Chunking algorithm and pre-summary -------------------------------------------
set.seed(123)
groupsize = 5
km.res <- kmeans(vectors, groupsize, nstart = 25)
df2 <- cbind(df, cluster = km.res$cluster)
presummarydf = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(df2)) {
document = df2[x,1]
source_python("aa_summarization.py")
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
presummarydf = rbind(presummarydf, summary)
}
for (x in 1:nrow(presummarydf)) {
stringtosum = paste(stringtosum, presummarydf[x,1], sep = " ", collapse = " ")
}
sumtokenoutput = 0
sumtokenoutput = sumtokenoutput + count_tokens(stringtosum)
df2 <- cbind(df2, presummarydf)
colnames(df2) <- c("Text_chunk", "cluster", "Summary")
View(presummarydf)
View(presummarydf)
View(df2)
View(df2)
df2[3,1]
df2[3,2]
df2[1,3]
df2[2,3]
df2[4,3]
df2[5,3]
# Install and or load packages - set variables ---------------------------------
packages <- c("pdftools","reticulate")
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
library(pdftools)
library(reticulate)
library(TheOpenAIR)
library(ds4psy)
library(glue)
# Initialize virtual ENV for Python
Sys.setenv(RETICULATE_PYTHON_ENV = "py_backend")
version <- "3.11"
virtualenv_create(python = virtualenv_starter(version))
virtualenv_install(requirements = "requirements.txt")
use_virtualenv("py_backend")
# Get your python file
source_python("aa_summarization.py")
source_python("aa_embedding.py")
outputpath = getwd()
inputpath = "Testfiles/1.pdf"
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzU1MH0.lqb0stSnX59IihBKdwISywfgK4amb6pBuhCNCSZqMnA"
#############################################
# Install and or load packages - set variables ---------------------------------
packages <- c("pdftools","reticulate")
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
library(pdftools)
library(reticulate)
library(TheOpenAIR)
library(ds4psy)
library(glue)
# Initialize virtual ENV for Python
Sys.setenv(RETICULATE_PYTHON_ENV = "py_backend")
version <- "3.11"
virtualenv_create(python = virtualenv_starter(version))
virtualenv_install(requirements = "requirements.txt")
use_virtualenv("py_backend")
# Get your python file
source_python("aa_summarization.py")
source_python("aa_embedding.py")
outputpath = getwd()
inputpath = "Testfiles/1.pdf"
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzU1MH0.lqb0stSnX59IihBKdwISywfgK4amb6pBuhCNCSZqMnA"
