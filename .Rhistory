outputpath = getwd()
inputpath = "Testfiles/1.pdf"
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6MzY1Mn0.CPMaAzjRd3iaS8e-mM-W6PysnmZoJyPm9C5MEiuGQlg"
################################################################################
# Short text summarization
################################################################################
# Load PDF file and parse ------------------------------------------------------
txt = pdf_text(inputpath)
document = txt[2] # select the page you want to summarize --------> SEITE SETZEN
maximum_tokens = 170
# Call Aleph Alpha API ---------------------------------------------------------
summary = summary(token, document, as.integer(maximum_tokens))
# Call Aleph Alpha API ---------------------------------------------------------
maximum_tokens = 170
summary = summary(token, document, as.integer(maximum_tokens))
summary = summary(token, document, as.integer(maximum_tokens))
summary = summary(token, document, as.integer(maximum_tokens))
summary
summary = summary(token, document, as.integer(maximum_tokens))
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6Mzc1Mn0.cbRae9zCWMLMsXRPWR_9x8z4Jl5y8sMSuPeCyaZGFuw"
summary = summary(token, document, as.integer(maximum_tokens))
summary
################################################################################
# Long text summarization
################################################################################
# Load PDF file and parse ------------------------------------------------------
# This step can be optimized by checking better OCR parsing libraries or adding procedures beforehand
txt = pdf_text(inputpath)
# processing of PDF file
stringtosum = ""
for (x in 1:length(txt)) {
stringtosum = paste(stringtosum, txt[x], sep = " ", collapse = " ")
}
stringtosum
dft = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
dft = dft[-1,]
View(dft)
# Get your python file
source_python("aa_tokenizer.py")
# count tokens per page
dft = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
tokens = aa_tokenizer(txt[x])
# tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
# Get your python file
source_python("aa_tokenizer.py")
x = 1
for (x in 1:length(txt)) {
tokens = aa_tokenizer(txt[x])
# tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
for (x in 1:length(txt)) {
tokens = aa_tokenizer(txt[x])
# tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
# Get your python file
source_python("aa_tokenizer.py")
for (x in 1:length(txt)) {
tokens = aa_tokenizer(txt[x])
# tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
# Install and or load packages - set variables ---------------------------------
packages <- c("pdftools","reticulate")
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
install.packages(packages[!installed_packages])
}
library(pdftools)
library(reticulate)
library(TheOpenAIR)
library(ds4psy)
library(glue)
# Initialize virtual ENV for Python
Sys.setenv(RETICULATE_PYTHON_ENV = "py_backend")
version <- "3.11"
virtualenv_create(python = virtualenv_starter(version))
virtualenv_install(requirements = "requirements.txt")
use_virtualenv("py_backend")
# Get your python file
source_python("aa_tokenizer.py")
source_python("aa_summarization.py")
source_python("aa_embedding.py")
outputpath = getwd()
inputpath = "Testfiles/1.pdf"
token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjo0MjcyLCJ0b2tlbl9pZCI6Mzc1Mn0.cbRae9zCWMLMsXRPWR_9x8z4Jl5y8sMSuPeCyaZGFuw"
# count tokens per page
dft = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
tokens = aa_tokenizer(token, txt[x])
# tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
# Get your python file
source_python("aa_tokenizer.py")
for (x in 1:length(txt)) {
tokens = aa_tokenizer(token, txt[x])
# tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
View(dft)
# count tokens per page
dft2 = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
#tokens = aa_tokenizer(token, txt[x])
tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
View(dft2)
################################################################################
# Long text summarization
################################################################################
# Load PDF file and parse ------------------------------------------------------
# This step can be optimized by checking better OCR parsing libraries or adding procedures beforehand
txt = pdf_text(inputpath)
# count tokens per page
dft2 = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
#tokens = aa_tokenizer(token, txt[x])
tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
# count tokens per page
dft2 = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
#tokens = aa_tokenizer(token, txt[x])
tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft2 = rbind(dft, tupel)
}
View(dft2)
# count tokens per page
dft2 = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
#tokens = aa_tokenizer(token, txt[x])
tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft2 = rbind(dft, tupel)
}
a
# count tokens per page
dft = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
#tokens = aa_tokenizer(token, txt[x])
tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
View(dft2)
View(dft)
for (x in 1:length(txt)) {
#tokens = aa_tokenizer(token, txt[x])
tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
# count tokens per page
dft = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
#tokens = aa_tokenizer(token, txt[x])
tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
dft = dft[-1,]
View(dft)
# count tokens per page
dft = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
#tokens = aa_tokenizer(token, txt[x])
string = gsub("[\r\n]", "", txt[x]) # remove new lines
tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
dft = dft[-1,]
View(dft)
# count tokens per page
dft = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
tokens = aa_tokenizer(token, txt[x])
string = gsub("[\r\n]", "", txt[x]) # remove new lines
#tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
dft = dft[-1,]
# count tokens per page
dft = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
string = gsub("[\r\n]", "", txt[x]) # remove new lines
tokens = aa_tokenizer(token, string)
#tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
dft = dft[-1,]
View(dft)
# count tokens per page
dft = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
string = gsub("[\r\n]", "", txt[x]) # remove new lines
#tokens = aa_tokenizer(token, string)
tokens = count_tokens(txt[x])
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
dft = dft[-1,]
View(dft)
# count tokens per page
dft = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
string = gsub("[\r\n]", "", txt[x]) # remove new lines
#tokens = aa_tokenizer(token, string)
tokens = count_tokens(string)
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
dft = dft[-1,]
View(dft)
# count tokens per page
dft = data.frame(page="", tokens="")
x = 1
for (x in 1:length(txt)) {
string = gsub("[\r\n]", "", txt[x]) # remove new lines
#tokens = aa_tokenizer(token, string)
tokens = count_tokens(string)
tupel = as.integer(c(x, tokens))
dft = rbind(dft, tupel)
}
dft = dft[-1,]
View(dft)
txt[1]
string = gsub("[\r\n]", "", txt[1]) # remove new lines
string
#tokens = aa_tokenizer(token, string)
tokens = count_tokens(string)
tokens
#tokens = aa_tokenizer(token, string)
tokens = count_tokens(txt[1])
tokens
dft$tokens <- as.integer(dft$tokens)
sumtokeninput <- sum(dft[, 'tokens'])
sumtokeninput
# Chunking algorithm -----------------------------------------------------------
string = paste(txt, sep="", collapse="")
string = gsub("[\r\n]", "", string) # remove new lines
dftext = text_to_sentences(string, split_delim = "\\.")  # only split at "."
dftext = as.data.frame(dftext)
View(dftext)
colnames(dftext) = "chunks"
View(dftext)
chunksize = 5
overlap = 1
cutsize = chunksize - overlap
iterations = round(nrow(dftext)/cutsize)-1
df = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:iterations){
chunk = paste(dftext[c(1:chunksize),], sep="", collapse="")
df = rbind(df, chunk)
dftext = as.data.frame(dftext[-c(1:cutsize), ])
}
View(dftext)
if (nrow(dftext) > 0) {
chunk = paste(dftext[c(1:nrow(dftext)),], sep="", collapse="")
df = rbind(df, chunk)
}
colnames(df) <- c("Text_chunk")
View(df)
df
View(df)
# Clustering algorithm ---------------------------------------------------------
vectors = embedding(token, text_chunks)
text_chunks = as.list(df[,1])
# Clustering algorithm ---------------------------------------------------------
vectors = embedding(token, text_chunks)
View(vectors)
vectors
vectors = matrix(unlist(vectors), ncol = 5120, byrow = TRUE)
View(vectors)
fviz_nbclust(vectors, kmeans, method = "wss") +
geom_vline(xintercept = "", linetype = 2)
fviz_nbclust(vectors, kmeans, method = "wss") +
geom_vline(xintercept = "", linetype = 2)
library(factoextra)
install.packages("factoextra")
library(factoextra)
# Initialize virtual ENV for Python
Sys.setenv(RETICULATE_PYTHON_ENV = "py_backend")
fviz_nbclust(vectors, kmeans, method = "wss") +
geom_vline(xintercept = "", linetype = 2)
fviz_nbclust(vectors, kmeans, method = "silhouette") +
geom_vline(xintercept = "", linetype = 2)
fviz_nbclust(vectors, kmeans, method = "silhouette") +
geom_vline(xintercept = "", linetype = 2)
fviz_nbclust(vectors, kmeans, method = "silhouette") +
geom_vline(xintercept = "", linetype = 2)
# Clustering algorithm ---------------------------------------------------------
set.seed(123)
groupsize = 2
km.res <- kmeans(vectors, groupsize, nstart = 25)
df2 <- cbind(df, cluster = km.res$cluster)
View(df2)
presummarydf = data.frame(matrix(nrow = 0, ncol = 0))
for (x in 1:nrow(df2)) {
document = df2[x,1]
source_python("aa_summarization.py")
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
presummarydf = rbind(presummarydf, summary)
}
View(presummarydf)
for (x in 1:nrow(presummarydf)) {
stringtosum = paste(stringtosum, presummarydf[x,1], sep = " ", collapse = " ")
}
sumtokenoutput = 0
sumtokenoutput = sumtokenoutput + count_tokens(stringtosum)
df2 <- cbind(df2, presummarydf)
colnames(df2) <- c("Text_chunk", "cluster", "Summary")
View(df2)
View(df2)
getwd()
write.csv(df,file=glue("{getwd()}/new_file.csv"), row.names=FALSE)
write.csv(df2,file=glue("{getwd()}/new_file.csv"), row.names=FALSE)
write.csv(df2,file=glue("{getwd()}/new_file.csv"), row.names=FALSE, sep = ";")
write.csv(df2,file=glue("{getwd()}/new_file.csv"), row.names=FALSE, fileEncoding = "UTF-8")
df2[2,1]
df2[2,2]
df2[3,2]
df2[3,1]
df2[1,1]
df2[2,1]
df2[2,2]
df2[2,3]
df2[3,3]
df2[3,1]
df2[5,1]
df2[5,3]
# Create summary per cluster ---------------------------------------------------
clustersummarydf = data.frame(matrix(nrow = 0, ncol = 0))
x = 0
for (x in 1:groupsize) {
#x = 2
group = df2[df2$cluster == x,]
document = ""
stringtosum = ""
for (y in 1:nrow(group)) {
stringtosum = paste(stringtosum, group[y,3], sep = " ", collapse = " ")
document = stringtosum
}
clustersummarydf = rbind(clustersummarydf, document)
}
colnames(clustersummarydf) <- c("Clustersummary")
View(clustersummarydf)
stringtosum
clustersummarydf
View(presummarydf)
colnames(df2) <- c("Text_chunk", "cluster", "Summary")
View(df2)
# Create summary per cluster ---------------------------------------------------
clustersummarydf = data.frame(matrix(nrow = 0, ncol = 0))
x = 0
for (x in 1:groupsize) {
#x = 2
group = df2[df2$cluster == x,]
document = ""
stringtosum = ""
for (y in 1:nrow(group)) {
stringtosum = paste(stringtosum, group[y,3], sep = " ", collapse = " ")
document = stringtosum
}
clustersummarydf = rbind(clustersummarydf, document)
}
colnames(clustersummarydf) <- c("Clustersummary")
View(clustersummarydf)
clustersummarydf
finalsummarydf = data.frame(matrix(nrow = 0, ncol = 0))
maximum_tokens = 124
for (x in 1:nrow(clustersummarydf)) {
document = clustersummarydf[x,1]
source_python("aa_summarization.py")
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
finalsummarydf = rbind(finalsummarydf, summary)
}
colnames(finalsummarydf) <- c("Finalsummary")
finalsummarydf
View(finalsummarydf)
maximum_tokens = 350
for (x in 1:nrow(clustersummarydf)) {
document = clustersummarydf[x,1]
source_python("aa_summarization.py")
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
finalsummarydf = rbind(finalsummarydf, summary)
}
colnames(finalsummarydf) <- c("Finalsummary")
finalsummarydf
finalsummarydf[1,1]
finalsummarydf = data.frame(matrix(nrow = 0, ncol = 0))
maximum_tokens = 350
for (x in 1:nrow(clustersummarydf)) {
document = clustersummarydf[x,1]
source_python("aa_summarization.py")
summary = summary(token, document, as.integer(maximum_tokens))
print(summary)
finalsummarydf = rbind(finalsummarydf, summary)
}
colnames(finalsummarydf) <- c("Finalsummary")
finalsummarydf[1,1]
finalsummarydf
clustersummarydf[1,1]
# Guided summary ---------------------------------------------------------------
document = ""
stringtosum = ""
for (x in 1:nrow(finalsummarydf)) {
stringtosum = paste(stringtosum, finalsummarydf[x,1], sep = " ", collapse = " ")
document = stringtosum
}
maximum_tokens = 350
question1 = "Was ist der Hintergrund?"
source_python("aa_summarization_guided.py")
background = summaryguided(token, document, question1, as.integer(maximum_tokens))
background
for (x in 1:nrow(clustersummarydf)) {
stringtosum = paste(stringtosum, finalsummarydf[x,1], sep = " ", collapse = " ")
document = stringtosum
}
# Guided summary ---------------------------------------------------------------
document = ""
stringtosum = ""
for (x in 1:nrow(clustersummarydf)) {
stringtosum = paste(stringtosum, finalsummarydf[x,1], sep = " ", collapse = " ")
document = stringtosum
}
maximum_tokens = 350
question1 = "Was ist der Hintergrund?"
source_python("aa_summarization_guided.py")
background = summaryguided(token, document, question1, as.integer(maximum_tokens))
background
clustersummarydf
# Guided summary ---------------------------------------------------------------
document = ""
stringtosum = ""
for (x in 1:nrow(clustersummarydf)) {
stringtosum = paste(stringtosum, finalsummarydf[x,1], sep = " ", collapse = " ")
document = stringtosum
}
maximum_tokens = 350
document
stringtosum = ""
for (x in 1:nrow(clustersummarydf)) {
stringtosum = paste(stringtosum, clustersummarydf[x,1], sep = " ", collapse = " ")
document = stringtosum
}
maximum_tokens = 350
question1 = "Was ist der Hintergrund?"
source_python("aa_summarization_guided.py")
background = summaryguided(token, document, question1, as.integer(maximum_tokens))
background
View(finalsummarydf)
question2 = "Ist das Urteil aus Sicht der Krankenkasse für die Abrechnungsprüfung relevant? Ist der Rechnungsbetrag zu kürzen? Wenn ja, aus welchem Grund?"
resolution = summaryguided(token, document, question2, as.integer(maximum_tokens))
resolution
guidedsummary = data.frame(matrix(nrow = 0, ncol = 3))
vector1 = cbind("Background", question1, background)
vector2 = cbind("Resolution", question2, resolution)
df3 = as.data.frame(rbind(vector1, vector2))
View(df3)
colnames(df3) <- c("Part","Question","Summary")
df3
write.csv(df3,file=glue("{getwd()}/new_file.csv"), row.names=FALSE, fileEncoding = "UTF-8")
df3[1,1]
df3[1,2]
df3[1,3]
df3[2,1]
df3[2,2]
df3[2,3]
renv:snapshot()
renv::snapshot()
renv::snapshot()
